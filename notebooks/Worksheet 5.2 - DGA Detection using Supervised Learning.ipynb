{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/GTK_Logo_Social Icon.jpg\" width=175 align=\"right\" />\n",
    "\n",
    "# Worksheet 5.2: DGA Detection\n",
    "This worksheet covers concepts covered in the second part of Module 5 - Supervised Learning.  It should take no more than 40-60 minutes to complete.  Please raise your hand if you get stuck.  \n",
    "\n",
    "## Import the Libraries\n",
    "For this exercise, we will be using:\n",
    "* Pandas (http://pandas.pydata.org/pandas-docs/stable/)\n",
    "* Numpy (https://docs.scipy.org/doc/numpy/reference/)\n",
    "* Matplotlib (http://matplotlib.org/api/pyplot_api.html)\n",
    "* Scikit-learn (http://scikit-learn.org/stable/documentation.html)\n",
    "* YellowBrick (http://www.scikit-yb.org/en/latest/)\n",
    "* Seaborn (https://seaborn.pydata.org)\n",
    "* Lime (https://github.com/marcotcr/lime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T14:28:17.893561Z",
     "start_time": "2020-07-30T14:28:16.461669Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cgivre/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load Libraries - Make sure to run this cell!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn import feature_extraction, tree, model_selection, metrics\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import lime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worksheet - Answer - DGA Detection using Machine Learning\n",
    "\n",
    "This worksheet is a step-by-step guide on how to detect domains that were generated using \"Domain Generation Algorithm\" (DGA). We will walk you through the process of transforming raw domain strings to Machine Learning features and creating a decision tree classifer which you will use to determine whether a given domain is legit or not. Once you have implemented the classifier, the worksheet will walk you through evaluating your model.  \n",
    "\n",
    "Overview 2 main steps:\n",
    "\n",
    "1. **Feature Engineering** - from raw domain strings to numeric Machine Learning features using DataFrame manipulations\n",
    "2. **Machine Learning Classification** - predict whether a domain is legit or not using a Decision Tree Classifier\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "**DGA - Background**\n",
    "\n",
    "\"Various families of malware use domain generation\n",
    "algorithms (DGAs) to generate a large number of pseudo-random\n",
    "domain names to connect to a command and control (C2) server.\n",
    "In order to block DGA C2 traffic, security organizations must\n",
    "first discover the algorithm by reverse engineering malware\n",
    "samples, then generate a list of domains for a given seed. The\n",
    "domains are then either preregistered, sink-holed or published\n",
    "in a DNS blacklist. This process is not only tedious, but can\n",
    "be readily circumvented by malware authors. An alternative\n",
    "approach to stop malware from using DGAs is to intercept DNS\n",
    "queries on a network and predict whether domains are DGA\n",
    "generated. Much of the previous work in DGA detection is based\n",
    "on finding groupings of like domains and using their statistical\n",
    "properties to determine if they are DGA generated. However,\n",
    "these techniques are run over large time windows and cannot be\n",
    "used for real-time detection and prevention. In addition, many of\n",
    "these techniques also use contextual information such as passive\n",
    "DNS and aggregations of all NXDomains throughout a network.\n",
    "Such requirements are not only costly to integrate, they may not\n",
    "be possible due to real-world constraints of many systems (such\n",
    "as endpoint detection). An alternative to these systems is a much\n",
    "harder problem: detect DGA generation on a per domain basis\n",
    "with no information except for the domain name. Previous work\n",
    "to solve this harder problem exhibits poor performance and many\n",
    "of these systems rely heavily on manual creation of features;\n",
    "a time consuming process that can easily be circumvented by\n",
    "malware authors...\"    \n",
    "[Citation: Woodbridge et. al 2016: \"Predicting Domain Generation Algorithms with Long Short-Term Memory Networks\"]\n",
    "\n",
    "A better alternative for real-world deployment would be to use \"featureless deep learning\" - We have a separate notebook where you can see how this can be implemented!\n",
    "\n",
    "**However, let's learn the basics first!!!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worksheet for Part 2 - Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakpoint: Load Features and Labels\n",
    "\n",
    "If you got stuck in Part 1, please simply load the feature matrix we prepared for you, so you can move on to Part 2 and train a Decision Tree Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv('../data/dga_features_final_df.csv')\n",
    "#If you didn't get a working dataset, uncomment this line\n",
    "#df_final = pd.read_csv('../data/our_data_dga_features_final_df.csv')\n",
    "\n",
    "print(df_final.isDGA.value_counts())\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionary of common english words from part 1\n",
    "from six.moves import cPickle as pickle\n",
    "with open('../data/d_common_en_words' + '.pickle', 'rb') as f:\n",
    "        d = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Machine Learning\n",
    "\n",
    "To learn simple classification procedures using [sklearn](http://scikit-learn.org/stable/) we have split the work flow into 5 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prepare Feature matrix and ```target``` vector containing the URL labels\n",
    "\n",
    "- In statistics, the feature matrix is often referred to as ```X```\n",
    "- target is a vector containing the labels for each URL (often also called *y* in statistics)\n",
    "- In sklearn both the input and target can either be a pandas DataFrame/Series or numpy array/vector respectively (can't be lists!)\n",
    "\n",
    "Tasks:\n",
    "- assign 'isDGA' column to a pandas Series named 'target'\n",
    "- drop 'isDGA' column from ```dga``` DataFrame and name the resulting pandas DataFrame 'feature_matrix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Simple Cross-Validation\n",
    "\n",
    "Tasks:\n",
    "- split your feature matrix X and target vector into train and test subsets using sklearn [model_selection.train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Cross-Validation: Split the data set into training and test data\n",
    "# Your code here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Train the model and make a prediction\n",
    "\n",
    "Finally, we have prepared and segmented the data. Let's start classifying!!   \n",
    "\n",
    "Tasks:\n",
    "\n",
    "-  Use the sklearn [tree.DecisionTreeClassfier()](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), create a decision tree with standard parameters, and train it using the ```.fit()``` function with ```X_train``` and ```target_train``` data.\n",
    "-  Next, pull a few random rows from the data and see if your classifier got it correct.\n",
    "\n",
    "If you are interested in trying a real unknown domain, you'll have to create a function to generate the features for that domain before you run it through the classifier (see function ```is_dga``` a few cells below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the decision tree based on the entropy criterion\n",
    "\n",
    "# Extract a row from the test data\n",
    "\n",
    "# Make the prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a Prediction\n",
    "The code below demonstrates how you will go from an unknown raw domain to predicting whether it is DGA or not.  The key thing is that you have to regenerate all the features, and create a 1 row dataframe of all your features which is then passed to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity let's just copy the needed function in here again\n",
    "\n",
    "def H_entropy (x):\n",
    "    # Calculate Shannon Entropy\n",
    "    prob = [ float(x.count(c)) / len(x) for c in dict.fromkeys(list(x)) ] \n",
    "    H = - sum([ p * np.log2(p) for p in prob ]) \n",
    "    return H\n",
    "\n",
    "def firstDigitIndex( s ):\n",
    "    for i, c in enumerate(s):\n",
    "        if c.isdigit():\n",
    "            return i + 1\n",
    "    return 0\n",
    "\n",
    "def vowel_consonant_ratio (x):\n",
    "    # Calculate vowel to consonant ratio\n",
    "    x = x.lower()\n",
    "    vowels_pattern = re.compile('([aeiou])')\n",
    "    consonants_pattern = re.compile('([b-df-hj-np-tv-z])')\n",
    "    vowels = re.findall(vowels_pattern, x)\n",
    "    consonants = re.findall(consonants_pattern, x)\n",
    "    try:\n",
    "        ratio = len(vowels) / len(consonants)\n",
    "    except: # catch zero devision exception \n",
    "        ratio = 0  \n",
    "    return ratio\n",
    "\n",
    "# ngrams: Implementation according to Schiavoni 2014: \"Phoenix: DGA-based Botnet Tracking and Intelligence\"\n",
    "# http://s2lab.isg.rhul.ac.uk/papers/files/dimva2014.pdf\n",
    "\n",
    "def ngrams(word, n):\n",
    "    # Extract all ngrams and return a regular Python list\n",
    "    # Input word: can be a simple string or a list of strings\n",
    "    # Input n: Can be one integer or a list of integers \n",
    "    # if you want to extract multipe ngrams and have them all in one list\n",
    "    \n",
    "    l_ngrams = []\n",
    "    if isinstance(word, list):\n",
    "        for w in word:\n",
    "            if isinstance(n, list):\n",
    "                for curr_n in n:\n",
    "                    ngrams = [w[i:i+curr_n] for i in range(0,len(w)-curr_n+1)]\n",
    "                    l_ngrams.extend(ngrams)\n",
    "            else:\n",
    "                ngrams = [w[i:i+n] for i in range(0,len(w)-n+1)]\n",
    "                l_ngrams.extend(ngrams)\n",
    "    else:\n",
    "        if isinstance(n, list):\n",
    "            for curr_n in n:\n",
    "                ngrams = [word[i:i+curr_n] for i in range(0,len(word)-curr_n+1)]\n",
    "                l_ngrams.extend(ngrams)\n",
    "        else:\n",
    "            ngrams = [word[i:i+n] for i in range(0,len(word)-n+1)]\n",
    "            l_ngrams.extend(ngrams)\n",
    "#     print(l_ngrams)\n",
    "    return l_ngrams\n",
    "\n",
    "def ngram_feature(domain, d, n):\n",
    "    # Input is your domain string or list of domain strings\n",
    "    # a dictionary object d that contains the count for most common english words\n",
    "    # finally you n either as int list or simple int defining the ngram length\n",
    "    \n",
    "    # Core magic: Looks up domain ngrams in english dictionary ngrams and sums up the \n",
    "    # respective english dictionary counts for the respective domain ngram\n",
    "    # sum is normalized\n",
    "    \n",
    "    l_ngrams = ngrams(domain, n)\n",
    "#     print(l_ngrams)\n",
    "    count_sum=0\n",
    "    for ngram in l_ngrams:\n",
    "        if d[ngram]:\n",
    "            count_sum+=d[ngram]\n",
    "    try:\n",
    "        feature = count_sum/(len(domain)-n+1)\n",
    "    except:\n",
    "        feature = 0\n",
    "    return feature\n",
    "    \n",
    "def average_ngram_feature(l_ngram_feature):\n",
    "    # input is a list of calls to ngram_feature(domain, d, n)\n",
    "    # usually you would use various n values, like 1,2,3...\n",
    "    return sum(l_ngram_feature)/len(l_ngram_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_dga(domain, clf, d):\n",
    "    # Function that takes new domain string, trained model 'clf' as input and\n",
    "    # dictionary d of most common english words\n",
    "    # returns prediction\n",
    "    \n",
    "    domain_features = np.empty([1,6])\n",
    "    # order of features is ['length', 'digits', 'entropy', 'vowel-cons', firstDigitIndex, 'ngrams']\n",
    "    domain_features[0,0] = len(domain)\n",
    "    pattern = re.compile('([0-9])')\n",
    "    domain_features[0,1] = len(re.findall(pattern, domain))\n",
    "    domain_features[0,2] = H_entropy(domain)\n",
    "    domain_features[0,3] = vowel_consonant_ratio(domain)\n",
    "    domain_features[0,4] = firstDigitIndex(domain)\n",
    "    domain_features[0,5] = average_ngram_feature([ngram_feature(domain, d, 1), \n",
    "                                                  ngram_feature(domain, d, 2), \n",
    "                                                  ngram_feature(domain, d, 3)])\n",
    "    pred = clf.predict(domain_features)\n",
    "    return pred[0]\n",
    "\n",
    "\n",
    "print('Predictions of domain %s is [0 means legit and 1 dga]: ' %('spardeingeld'), is_dga('spardeingeld', clf, d))  \n",
    "print('Predictions of domain %s is [0 means legit and 1 dga]: ' %('google'), is_dga('google', clf, d)) \n",
    "print('Predictions of domain %s is [0 means legit and 1 dga]: ' %('1vxznov16031kjxneqjk1rtofi6'), is_dga('1vxznov16031kjxneqjk1rtofi6', clf, d)) \n",
    "print('Predictions of domain %s is [0 means legit and 1 dga]: ' %('lthmqglxwmrwex'), is_dga('lthmqglxwmrwex', clf, d)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Assess model accuracy with simple cross-validation\n",
    "\n",
    "Tasks:\n",
    "- Make predictions for all your data. Call the ```.predict()``` method on the clf with your training data ```X_train``` and store the results in a variable called ```target_pred```.\n",
    "- Use sklearn [metrics.accuracy_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) to determine your models accuracy. Detailed Instruction:\n",
    "    - Use your trained model to predict the labels of your test data ```X_test```. Run ```.predict()``` method on the clf with your test data ```X_test``` and store the results in a variable called ```target_pred```.. \n",
    "    - Then calculate the accuracy using ```target_test``` (which are the true labels/groundtruth) AND your models predictions on the test portion ```target_pred``` as inputs. The advantage here is to see how your model performs on new data it has not been seen during the training phase. The fair approach here is a simple **cross-validation**!\n",
    "    \n",
    "- Print out the confusion matrix using [metrics.confusion_matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "- Use Yellowbrick to visualize the classification report and confusion matrix. (http://www.scikit-yb.org/en/latest/examples/modelselect.html#common-metrics-for-evaluating-classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fair approach: make prediction on test data portion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report...neat summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Assess model accuracy with k-fold cross-validation\n",
    "\n",
    "Tasks:\n",
    "- Partition the dataset into *k* different subsets\n",
    "- Create *k* different models by training on *k-1* subsets and testing on the remaining subsets\n",
    "- Measure the performance on each of the models and take the average measure.\n",
    "\n",
    "*Short-Cut*\n",
    "All of these steps can be easily achieved by simply using sklearn's [model_selection.KFold()](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) and [model_selection.cross_val_score()](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) Visualizing your Tree\n",
    "As an optional step, you can actually visualize your tree.  The following code will generate a graph of your decision tree.  You will need graphviz (http://www.graphviz.org) and pydotplus (or pydot) installed for this to work.\n",
    "The Griffon VM has this installed already, but if you try this on a Mac, or Linux machine you will need to install graphviz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These libraries are used to visualize the decision tree and require that you have GraphViz\n",
    "# and pydot or pydotplus installed on your computer.\n",
    "\n",
    "from IPython.core.display import Image\n",
    "import pydotplus as pydot\n",
    "\n",
    "\n",
    "dot_data = io.StringIO() \n",
    "tree.export_graphviz(clf, out_file=dot_data, \n",
    "                     feature_names=['length', 'digits', 'entropy', 'vowel-cons', 'firstDigitIndex','ngrams'],\n",
    "                    filled=True, rounded=True,  \n",
    "                    special_characters=True) \n",
    "\n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue()) \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Models\n",
    "Now that you've built a Decision Tree, let's try out two other classifiers and see how they perform on this data.  For this next exercise, create classifiers using:\n",
    "\n",
    "* Support Vector Machine\n",
    "* Random Forest\n",
    "* K-Nearest Neighbors (http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)  \n",
    "\n",
    "Once you've done that, run the various performance metrics to determine which classifier works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, create the SVM classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally the knn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain a Prediction\n",
    "In the example below, you can use LIME to explain how a classifier arrived at its prediction.  Try running LIME with the various classifiers you've created and various rows to see how it functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime.lime_tabular\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(feature_matrix_train, \n",
    "                                                   feature_names=['length', 'digits', 'entropy', 'vowel-cons', 'firstDigitIndex','ngrams'], \n",
    "                                                   class_names=['legit', 'isDGA'], \n",
    "                                                   discretize_continuous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(feature_matrix_test.iloc[5], \n",
    "                                 random_forest_clf.predict_proba, \n",
    "                                 num_features=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(show_table=True, show_all=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
